{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMwn64/fmrMygM+s2DKQFBB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/intimanjunath/AI-Agents-Design-Patterns-using-LangGraph/blob/main/LangGraph_Agent_Design_Patterns_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Your LangSmith API key\n",
        "os.environ[\"LANGCHAIN_API_KEY\"]      = \"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\"\n",
        "\n",
        "# Enable the new tracing system\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]   = \"true\"\n",
        "\n",
        "# (Optional) Project and session naming—helps you organize traces in the UI\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]      = \"AgentPatternsDemo\"\n",
        "os.environ[\"LANGCHAIN_SESSION_NAME\"] = \"PromptChaining_LangGraph\""
      ],
      "metadata": {
        "id": "c6PD8cWMfKv4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pattern 1️⃣: Prompt Chaining\n",
        "A classic pipeline where each LLM output feeds directly into the next step."
      ],
      "metadata": {
        "id": "RNVOxQCIfJ7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from typing import TypedDict"
      ],
      "metadata": {
        "id": "r_cbDXpzJ6rO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the shared state schema\n",
        "class ChainState(TypedDict):\n",
        "    text: str\n",
        "    summary: str\n",
        "    translation: str\n",
        "    final_output: str\n",
        "\n",
        "# Step 1: Summarize\n",
        "def summarize(state: ChainState) -> ChainState:\n",
        "    summary = f\"This is a brief summary of: {state['text'][:60]}...\"\n",
        "    print(\"📝 Summarized\")\n",
        "    return {**state, \"summary\": summary}\n",
        "\n",
        "# Step 2: Translate to French\n",
        "def translate_to_french(state: ChainState) -> ChainState:\n",
        "    translated = f\"(French) Ceci est un résumé de: {state['summary']}\"\n",
        "    print(\"🌐 Translated\")\n",
        "    return {**state, \"translation\": translated}\n",
        "\n",
        "# Step 3: Add Greeting\n",
        "def add_greeting(state: ChainState) -> ChainState:\n",
        "    output = \"Bonjour! \" + state[\"translation\"]\n",
        "    print(\"👋 Added Greeting\")\n",
        "    return {**state, \"final_output\": output}\n"
      ],
      "metadata": {
        "id": "ir6_FooLKZ8O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(ChainState)\n",
        "builder.add_node(\"summarizer\", RunnableLambda(summarize))\n",
        "builder.add_node(\"translator\", RunnableLambda(translate_to_french))\n",
        "builder.add_node(\"greeting\", RunnableLambda(add_greeting))\n",
        "\n",
        "builder.set_entry_point(\"summarizer\")\n",
        "builder.add_edge(\"summarizer\", \"translator\")\n",
        "builder.add_edge(\"translator\", \"greeting\")\n",
        "builder.add_edge(\"greeting\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "8OwQOgQHKlHE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_state = {\n",
        "    \"text\":       \"Artificial Intelligence is transforming industries through automation, personalization, and data-driven decision-making.\",\n",
        "    \"summary\":    \"\",\n",
        "    \"translation\":\"\",\n",
        "    \"final_output\":\"\"\n",
        "}\n",
        "\n",
        "result = graph.invoke(input_state)\n",
        "print(\"\\n🧾 Final Output:\\n\", result[\"final_output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NVqCeOIKmsm",
        "outputId": "8edf664f-06e3-489f-8b8d-88cf04e7be1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Summarized\n",
            "🌐 Translated\n",
            "👋 Added Greeting\n",
            "\n",
            "🧾 Final Output:\n",
            " Bonjour! (French) Ceci est un résumé de: This is a brief summary of: Artificial Intelligence is transforming industries through a...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 2: Parallelization\n",
        "Let’s implement the Parallelization Pattern next. We'll use the ThreadPoolExecutor approach to simulate parallel LLM calls in a single node."
      ],
      "metadata": {
        "id": "NcCyVit5Ku1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update session for LangSmith tracing\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_SESSION_NAME\"] = \"Parallelization_LangGraph\"\n",
        "\n",
        "# Imports (if not already present)\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from typing import TypedDict\n",
        "from typing_extensions import Annotated\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "Of9L1s63K3u0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shared state schema\n",
        "class ParallelState(TypedDict):\n",
        "    query: Annotated[str, \"shared\"]\n",
        "    science_answer: str\n",
        "    analogy_answer: str\n",
        "    merged_output: str\n",
        "\n",
        "# Science Expert\n",
        "def explain_scientifically(state: ParallelState) -> ParallelState:\n",
        "    print(\"🔬 Science Expert responding...\")\n",
        "    return {\n",
        "        **state,\n",
        "        \"science_answer\": f\"{state['query']} causes global warming due to greenhouse gases.\"\n",
        "    }\n",
        "\n",
        "# Analogy Expert\n",
        "def explain_with_analogy(state: ParallelState) -> ParallelState:\n",
        "    print(\"🎨 Analogy Expert responding...\")\n",
        "    return {\n",
        "        **state,\n",
        "        \"analogy_answer\": f\"{state['query']} is like wrapping the Earth in a blanket that traps heat.\"\n",
        "    }\n",
        "\n",
        "# Parallel runner node\n",
        "def parallel_runner(state: ParallelState) -> ParallelState:\n",
        "    with ThreadPoolExecutor() as exe:\n",
        "        sci_fut = exe.submit(explain_scientifically, state)\n",
        "        ana_fut = exe.submit(explain_with_analogy, state)\n",
        "        sci_res = sci_fut.result()\n",
        "        ana_res = ana_fut.result()\n",
        "    return {\n",
        "        **state,\n",
        "        \"science_answer\": sci_res[\"science_answer\"],\n",
        "        \"analogy_answer\": ana_res[\"analogy_answer\"]\n",
        "    }\n",
        "\n",
        "# Merge responses\n",
        "def merge_responses(state: ParallelState) -> ParallelState:\n",
        "    print(\"🔗 Merging both responses...\")\n",
        "    combined = (\n",
        "        f\"Scientific Explanation:\\n{state['science_answer']}\\n\\n\"\n",
        "        f\"Analogy:\\n{state['analogy_answer']}\"\n",
        "    )\n",
        "    return {**state, \"merged_output\": combined}"
      ],
      "metadata": {
        "id": "q8JzBvkLKoGi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build & Compile Graph\n",
        "\n",
        "builder = StateGraph(ParallelState)\n",
        "builder.add_node(\"parallel\", RunnableLambda(parallel_runner))\n",
        "builder.add_node(\"merge\", RunnableLambda(merge_responses))\n",
        "\n",
        "builder.set_entry_point(\"parallel\")\n",
        "builder.add_edge(\"parallel\", \"merge\")\n",
        "builder.add_edge(\"merge\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "h6J2c0tAKyEg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Invoke & Inspect\n",
        "\n",
        "input_state = {\n",
        "    \"query\": \"Climate change\",\n",
        "    \"science_answer\": \"\",\n",
        "    \"analogy_answer\": \"\",\n",
        "    \"merged_output\": \"\"\n",
        "}\n",
        "\n",
        "result = graph.invoke(input_state)\n",
        "print(\"\\n🧾 Final Combined Output:\\n\", result[\"merged_output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPj4_XNuLLvy",
        "outputId": "1bda8b90-b51f-48d4-dc66-1658090a1524"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔬 Science Expert responding...\n",
            "🎨 Analogy Expert responding...\n",
            "🔗 Merging both responses...\n",
            "\n",
            "🧾 Final Combined Output:\n",
            " Scientific Explanation:\n",
            "Climate change causes global warming due to greenhouse gases.\n",
            "\n",
            "Analogy:\n",
            "Climate change is like wrapping the Earth in a blanket that traps heat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 3: Orchestrator–Worker Pattern. You’ll split a long text into two parts, have two workers summarize each, then synthesize the final summary."
      ],
      "metadata": {
        "id": "eoFp9oNkLarF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to Orchestrator–Worker session\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_SESSION_NAME\"] = \"OrchestratorWorker_LangGraph\"\n",
        "\n",
        "# Imports\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from typing import TypedDict"
      ],
      "metadata": {
        "id": "o-oPvDSjLOvC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "\n",
        "# State schema\n",
        "class TaskPlanState(TypedDict):\n",
        "    project_idea: str\n",
        "    backend_plan: str\n",
        "    frontend_plan: str\n",
        "    full_plan: str\n",
        "\n",
        "# Orchestrator: dispatch project into parts\n",
        "def orchestrator_task_split(state: TaskPlanState) -> TaskPlanState:\n",
        "    print(\"🧠 Orchestrator split idea into backend + frontend.\")\n",
        "    return {**state}\n",
        "\n",
        "# Backend worker\n",
        "def backend_worker(state: TaskPlanState) -> TaskPlanState:\n",
        "    print(\"🧑‍💻 Backend worker generating tasks...\")\n",
        "    backend = (\n",
        "        \"- Build intent classification model\\n\"\n",
        "        \"- Set up database (MongoDB)\\n\"\n",
        "        \"- Create API with FastAPI\\n\"\n",
        "        \"- Integrate logging and analytics\"\n",
        "    )\n",
        "    return {**state, \"backend_plan\": backend}\n",
        "\n",
        "# Frontend worker\n",
        "def frontend_worker(state: TaskPlanState) -> TaskPlanState:\n",
        "    print(\"🎨 Frontend worker generating tasks...\")\n",
        "    frontend = (\n",
        "        \"- Design chatbot UI (React)\\n\"\n",
        "        \"- Add voice input support\\n\"\n",
        "        \"- Integrate with backend API\\n\"\n",
        "        \"- Create responsive layout\"\n",
        "    )\n",
        "    return {**state, \"frontend_plan\": frontend}\n",
        "\n",
        "# Synthesizer: merge both plans\n",
        "def synthesize_task_plan(state: TaskPlanState) -> TaskPlanState:\n",
        "    print(\"🧾 Synthesizing full plan...\")\n",
        "    plan = f\"🧑‍💻 Backend:\\n{state['backend_plan']}\\n\\n🎨 Frontend:\\n{state['frontend_plan']}\"\n",
        "    return {**state, \"full_plan\": plan}"
      ],
      "metadata": {
        "id": "YA6l-nrmLfpX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "builder = StateGraph(TaskPlanState)\n",
        "builder.add_node(\"orchestrator\", RunnableLambda(orchestrator_task_split))\n",
        "builder.add_node(\"backend_worker\", RunnableLambda(backend_worker))\n",
        "builder.add_node(\"frontend_worker\", RunnableLambda(frontend_worker))\n",
        "builder.add_node(\"synthesizer\", RunnableLambda(synthesize_task_plan))\n",
        "\n",
        "builder.set_entry_point(\"orchestrator\")\n",
        "builder.add_edge(\"orchestrator\", \"backend_worker\")\n",
        "builder.add_edge(\"backend_worker\", \"frontend_worker\")\n",
        "builder.add_edge(\"frontend_worker\", \"synthesizer\")\n",
        "builder.add_edge(\"synthesizer\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "TH3vBOd9LmtU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_state = {\n",
        "    \"project_idea\": \"Build an AI-powered chatbot for customer service\",\n",
        "    \"backend_plan\": \"\",\n",
        "    \"frontend_plan\": \"\",\n",
        "    \"full_plan\": \"\"\n",
        "}\n",
        "\n",
        "result = graph.invoke(input_state)\n",
        "print(\"\\n🧾 Final Project Plan:\\n\")\n",
        "print(result[\"full_plan\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQOUbmzMLqUY",
        "outputId": "ad2724b5-00f4-474e-96f8-a6eb54ef6c12"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Orchestrator split idea into backend + frontend.\n",
            "🧑‍💻 Backend worker generating tasks...\n",
            "🎨 Frontend worker generating tasks...\n",
            "🧾 Synthesizing full plan...\n",
            "\n",
            "🧾 Final Project Plan:\n",
            "\n",
            "🧑‍💻 Backend:\n",
            "- Build intent classification model\n",
            "- Set up database (MongoDB)\n",
            "- Create API with FastAPI\n",
            "- Integrate logging and analytics\n",
            "\n",
            "🎨 Frontend:\n",
            "- Design chatbot UI (React)\n",
            "- Add voice input support\n",
            "- Integrate with backend API\n",
            "- Create responsive layout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Evaluator–Optimizer Pattern\n",
        "\n",
        "We’ll generate 3 taglines for an AI product, and score them for creativity (mocked).\n",
        "\n",
        "The agent will:\n",
        "\n",
        "🧠 Generator: Creates 3 taglines\n",
        "\n",
        "🧪 Evaluator: Assigns a creativity score to each\n",
        "\n",
        "🏆 Aggregator: Picks the highest-scoring tagline and formats output\n",
        "\n"
      ],
      "metadata": {
        "id": "8H9YavXZNbp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List\n",
        "from langgraph.graph import StateGraph, END, START\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# LangSmith session for this pattern\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_SESSION_NAME\"] = \"EvaluatorOptimizer_LangGraph\"\n",
        "\n",
        "# Graph state\n",
        "class EvalState(TypedDict):\n",
        "    product: str\n",
        "    taglines: List[str]\n",
        "    scores: List[float]\n",
        "    best_tagline: str\n",
        "    final_output: str"
      ],
      "metadata": {
        "id": "QZCPJ_2WLrzp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator\n",
        "def generate_taglines(state: EvalState) -> EvalState:\n",
        "    print(\"🧠 Generating taglines...\")\n",
        "    product = state[\"product\"]\n",
        "    taglines = [\n",
        "        f\"Talk to {product}, not just about it.\",\n",
        "        f\"{product}: Intelligence at your service.\",\n",
        "        f\"{product} that listens, learns, and responds.\"\n",
        "    ]\n",
        "    return {**state, \"taglines\": taglines}\n",
        "\n",
        "# Evaluator (mocked creativity score)\n",
        "def evaluate_taglines(state: EvalState) -> EvalState:\n",
        "    print(\"🧪 Evaluating taglines...\")\n",
        "    scores = [len(t) % 10 + 1 for t in state[\"taglines\"]]  # Fake scoring logic\n",
        "    return {**state, \"scores\": scores}\n",
        "\n",
        "# Aggregator: pick best tagline + format output\n",
        "def select_best_tagline(state: EvalState) -> EvalState:\n",
        "    print(\"🏆 Selecting best tagline...\")\n",
        "    best_index = state[\"scores\"].index(max(state[\"scores\"]))\n",
        "    best_tagline = state[\"taglines\"][best_index]\n",
        "\n",
        "    output = f\"🎯 Generated Taglines for '{state['product']}':\\n\"\n",
        "    for i, (tag, score) in enumerate(zip(state[\"taglines\"], state[\"scores\"]), 1):\n",
        "        output += f\"{i}. \\\"{tag}\\\" — Creativity Score: {score}/10\\n\"\n",
        "\n",
        "    output += f\"\\n🏆 Top Choice: \\\"{best_tagline}\\\"\"\n",
        "    return {**state, \"best_tagline\": best_tagline, \"final_output\": output}"
      ],
      "metadata": {
        "id": "2lyPWZ6GNhZh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build graph\n",
        "builder = StateGraph(EvalState)\n",
        "builder.add_node(\"generator\", RunnableLambda(generate_taglines))\n",
        "builder.add_node(\"evaluator\", RunnableLambda(evaluate_taglines))\n",
        "builder.add_node(\"aggregator\", RunnableLambda(select_best_tagline))\n",
        "\n",
        "builder.set_entry_point(\"generator\")\n",
        "builder.add_edge(\"generator\", \"evaluator\")\n",
        "builder.add_edge(\"evaluator\", \"aggregator\")\n",
        "builder.add_edge(\"aggregator\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "ibyOEukPNjXV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_state = {\n",
        "    \"product\": \"ChatMind AI\",\n",
        "    \"taglines\": [],\n",
        "    \"scores\": [],\n",
        "    \"best_tagline\": \"\",\n",
        "    \"final_output\": \"\"\n",
        "}\n",
        "\n",
        "result = graph.invoke(input_state)\n",
        "print(\"\\n🧾 Final Output:\\n\")\n",
        "print(result[\"final_output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeurpIyQNmjc",
        "outputId": "92e6faee-88cf-4d5b-e6b1-c20dd0f2f113"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Generating taglines...\n",
            "🧪 Evaluating taglines...\n",
            "🏆 Selecting best tagline...\n",
            "\n",
            "🧾 Final Output:\n",
            "\n",
            "🎯 Generated Taglines for 'ChatMind AI':\n",
            "1. \"Talk to ChatMind AI, not just about it.\" — Creativity Score: 10/10\n",
            "2. \"ChatMind AI: Intelligence at your service.\" — Creativity Score: 3/10\n",
            "3. \"ChatMind AI that listens, learns, and responds.\" — Creativity Score: 8/10\n",
            "\n",
            "🏆 Top Choice: \"Talk to ChatMind AI, not just about it.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next: Section 5 – Routing Pattern\n",
        "Would you like to continue to Routing now, where we:\n",
        "\n",
        "Accept a user query like “Tell me the news”\n",
        "\n",
        "Route it dynamically to either:\n",
        "\n",
        "🗞️ NewsAgent\n",
        "\n",
        "☀️ WeatherAgent\n",
        "\n",
        "🤖 FallbackAgent\n"
      ],
      "metadata": {
        "id": "SyucbZzmNx5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# LangSmith session\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_SESSION_NAME\"] = \"Routing_LangGraph\"\n",
        "\n",
        "# State definition\n",
        "class RouteState(TypedDict):\n",
        "    query: str\n",
        "    route: str\n",
        "    response: str\n",
        "    final_output: str"
      ],
      "metadata": {
        "id": "ghJpRAfNNoWe"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Router: detect intent\n",
        "def router_node(state: RouteState) -> RouteState:\n",
        "    print(\"🧭 Routing based on query...\")\n",
        "    q = state[\"query\"].lower()\n",
        "    if \"news\" in q:\n",
        "        return {**state, \"route\": \"news\"}\n",
        "    elif \"weather\" in q:\n",
        "        return {**state, \"route\": \"weather\"}\n",
        "    else:\n",
        "        return {**state, \"route\": \"fallback\"}\n",
        "\n",
        "# News Agent\n",
        "def news_agent(state: RouteState) -> RouteState:\n",
        "    print(\"🗞 News agent responding...\")\n",
        "    return {**state, \"response\": \"Breaking News: AI is now writing your code!\"}\n",
        "\n",
        "# Weather Agent\n",
        "def weather_agent(state: RouteState) -> RouteState:\n",
        "    print(\"☀️ Weather agent responding...\")\n",
        "    return {**state, \"response\": \"It's sunny and 24°C in your area.\"}\n",
        "\n",
        "# Fallback Agent\n",
        "def fallback_agent(state: RouteState) -> RouteState:\n",
        "    print(\"🤖 Fallback agent responding...\")\n",
        "    return {**state, \"response\": \"I'm not sure how to answer that, but I'm learning!\"}\n",
        "\n",
        "# Aggregator\n",
        "def route_aggregator(state: RouteState) -> RouteState:\n",
        "    icon = {\"news\": \"🗞\", \"weather\": \"☀️\", \"fallback\": \"🤖\"}.get(state[\"route\"], \"❓\")\n",
        "    result = f\"📬 Query: \\\"{state['query']}\\\"\\n{icon} Agent says: {state['response']}\"\n",
        "    return {**state, \"final_output\": result}"
      ],
      "metadata": {
        "id": "Qcjr4kXtN5aF"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(RouteState)\n",
        "\n",
        "# Add nodes\n",
        "builder.add_node(\"router\", RunnableLambda(router_node))\n",
        "builder.add_node(\"news\", RunnableLambda(news_agent))\n",
        "builder.add_node(\"weather\", RunnableLambda(weather_agent))\n",
        "builder.add_node(\"fallback\", RunnableLambda(fallback_agent))\n",
        "builder.add_node(\"aggregator\", RunnableLambda(route_aggregator))\n",
        "\n",
        "# Entry + conditional routing\n",
        "builder.set_entry_point(\"router\")\n",
        "builder.add_conditional_edges(\"router\", lambda state: state[\"route\"], {\n",
        "    \"news\": \"news\",\n",
        "    \"weather\": \"weather\",\n",
        "    \"fallback\": \"fallback\"\n",
        "})\n",
        "\n",
        "# Chain each agent to the aggregator\n",
        "builder.add_edge(\"news\", \"aggregator\")\n",
        "builder.add_edge(\"weather\", \"aggregator\")\n",
        "builder.add_edge(\"fallback\", \"aggregator\")\n",
        "builder.add_edge(\"aggregator\", END)\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "k34RXISTN7Jt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try different queries here:\n",
        "test_query = \"What's the weather like today?\"\n",
        "\n",
        "input_state = {\n",
        "    \"query\": test_query,\n",
        "    \"route\": \"\",\n",
        "    \"response\": \"\",\n",
        "    \"final_output\": \"\"\n",
        "}\n",
        "\n",
        "result = graph.invoke(input_state)\n",
        "\n",
        "print(\"\\n🧾 Final Output:\\n\")\n",
        "print(result[\"final_output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaHzfcCvN8fa",
        "outputId": "f8b9c067-0741-4405-a0c9-82c5ce9303db"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧭 Routing based on query...\n",
            "☀️ Weather agent responding...\n",
            "\n",
            "🧾 Final Output:\n",
            "\n",
            "📬 Query: \"What's the weather like today?\"\n",
            "☀️ Agent says: It's sunny and 24°C in your area.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 6: Autonomous Agent Loop Pattern\n",
        "🧠 Goal:\n",
        "Simulate an agent that:\n",
        "\n",
        "Picks a tool based on the task\n",
        "\n",
        "Uses the tool\n",
        "\n",
        "Gets feedback\n",
        "\n",
        "Repeats until goal is complete\n"
      ],
      "metadata": {
        "id": "Cm7UKM69OGLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_SESSION_NAME\"] = \"AutonomousAgent_Rewriter_Loop\"\n",
        "\n",
        "# Shared state\n",
        "class RewriteState(TypedDict):\n",
        "    text: str\n",
        "    rewritten: str\n",
        "    word_count: int\n",
        "    done: bool\n",
        "    final_output: str"
      ],
      "metadata": {
        "id": "gYyOFAmTN91f"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rewriter tool (mock LLM improving conciseness)\n",
        "def rewrite_tool(state: RewriteState) -> RewriteState:\n",
        "    print(\"✍️ Rewriting text...\")\n",
        "\n",
        "    original = state[\"rewritten\"] or state[\"text\"]\n",
        "\n",
        "    # Fake smarter rewriter (replace long phrases with short ones)\n",
        "    replacements = {\n",
        "        \"LangGraph allows for\": \"LangGraph enables\",\n",
        "        \"structured LLM workflows\": \"LLM pipelines\",\n",
        "        \"using graphs to coordinate decisions, tools, and memory\":\n",
        "            \"via graphs coordinating decisions\"\n",
        "    }\n",
        "\n",
        "    rewritten = original\n",
        "    for long, short in replacements.items():\n",
        "        rewritten = rewritten.replace(long, short)\n",
        "\n",
        "    return {**state, \"rewritten\": rewritten}\n",
        "\n",
        "# Evaluator tool: checks if rewritten version is short enough\n",
        "def evaluator_tool(state: RewriteState) -> RewriteState:\n",
        "    rewritten = state[\"rewritten\"]\n",
        "    count = len(rewritten.split())\n",
        "    print(f\"📏 Evaluating: {count} words.\")\n",
        "    done = count <= 20\n",
        "    output = (\n",
        "        f\"✅ Final rewritten version:\\n{rewritten}\"\n",
        "        if done else\n",
        "        f\"🔁 Still too long ({count} words). Retrying...\"\n",
        "    )\n",
        "    return {\n",
        "        **state,\n",
        "        \"word_count\": count,\n",
        "        \"done\": done,\n",
        "        \"final_output\": output if done else \"\"\n",
        "    }\n",
        "\n",
        "# Decision controller: loop until goal met\n",
        "def control_next(state: RewriteState):\n",
        "    print(\"🧠 Agent controller deciding next step...\")\n",
        "    return \"__end__\" if state[\"done\"] else \"rewrite\""
      ],
      "metadata": {
        "id": "nQgM4QPtOIiV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "builder = StateGraph(RewriteState)\n",
        "builder.add_node(\"rewrite\", RunnableLambda(rewrite_tool))\n",
        "builder.add_node(\"evaluate\", RunnableLambda(evaluator_tool))\n",
        "\n",
        "builder.set_entry_point(\"rewrite\")\n",
        "builder.add_edge(\"rewrite\", \"evaluate\")\n",
        "builder.add_conditional_edges(\"evaluate\", control_next, {\n",
        "    \"rewrite\": \"rewrite\",\n",
        "    \"__end__\": END\n",
        "})\n",
        "\n",
        "graph = builder.compile()"
      ],
      "metadata": {
        "id": "v3aoWzgROJ0S"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_state = {\n",
        "    \"text\": \"LangGraph allows for structured LLM workflows using graphs to coordinate decisions, tools, and memory.\",\n",
        "    \"rewritten\": \"\",\n",
        "    \"word_count\": 0,\n",
        "    \"done\": False,\n",
        "    \"final_output\": \"\"\n",
        "}\n",
        "\n",
        "result = graph.invoke(input_state)\n",
        "\n",
        "print(\"\\n🧾 Final Output:\\n\")\n",
        "print(result[\"final_output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAPhOAlKOLYY",
        "outputId": "0b38ba8c-dba4-4ffd-e782-7e3543ae392d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✍️ Rewriting text...\n",
            "📏 Evaluating: 8 words.\n",
            "🧠 Agent controller deciding next step...\n",
            "\n",
            "🧾 Final Output:\n",
            "\n",
            "✅ Final rewritten version:\n",
            "LangGraph enables LLM pipelines via graphs coordinating decisions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EjobE48-OM-H"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}